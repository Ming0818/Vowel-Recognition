---
title: "Vovel Recognition"
output: html_notebook
---




```{r,}
require(ElemStatLearn)#contains the dataset
require(keras)

```

Checking the dimentions of the data set and the summary statistics
```{r}
cat("The dimentions of Train data set is:",dim(vowel.train))
cat("The dimentions of Test data set is:",dim(vowel.test))

#checking the summary statistics
summary(vowel.train)



```


------------

####Preparing the Data to feed it to the  Keras Model

```{r}
train_x<-vowel.train[-1]#the inputs or predictors
train_y<-vowel.train[1]#the class labels of target vector

#the class labels are the 11 steady state vowels used in British English


#Test data
test_x<-vowel.test[-1]
test_y<-vowel.test[1]

#converting to matrix format
train_x<-as.matrix(train_x)
test_x<-as.matrix(test_x)
train_y<-as.matrix(train_y)
test_y<-as.matrix(test_y)

#comverting Target values to one-hot encoding i.e binary class matrix
train_y<-to_categorical(train_y,num_classes = 12) 
test_y<-to_categorical(test_y,num_classes = 12)


```

--------------


Now we need to define a Keras sequential model and its architecture with choosing the hyperparameters such as number of hidden units, optimizer to choose etc.

```{r}
mlp<-keras_model_sequential() # a keras sequential model


#defining the parameters and architecture
#adding layers to the model
mlp %>%
  #hidden layer=8 hidden units
  layer_dense(units=8,activation="relu",input_shape=c(10)) %>%
  #output layer
  layer_dense(units=12,activation="softmax") #to compute probabilities
  #softmax activation = exp(y)/exp(y_i)

summary(mlp) #model has 196 params

get_config(mlp)
mlp$layers
get_layer(mlp,index=1)

#information about the layers added
mlp$input
mlp$output

#defining the optimization strategy and the loss function to be used
mlp %>%compile(loss="categorical_crossentropy",
               optimizer="sgd",metrics="accuracy")


```

__Cross entropy__ is simply the negetive log likelihood of the probabilities of the predicted classes and the actual class labels.It is used as a Loss function in lots of Classifiers.In this case it is multiclass cross entropy as we are dealing with multiple classes.
$$ Entropy = -\sum_{i=1}^{K}t_i.log(y_i)$$
where $K$ are the number of classes and $t_i$ is the probability of actual $K$ class labels in the dataset, and $y_i$ is the predicted probabilities of the $K$ classes by the Model.


